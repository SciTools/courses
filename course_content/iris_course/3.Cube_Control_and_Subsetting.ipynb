{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris introduction course\n",
    "# 3. Cube Control and Subsetting\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to apply Iris functionality to take a useful subset of an Iris cube and to combine multiple Iris cubes into a new larger cube.\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**Overview:**<br>\n",
    "3.1 [Indexing and Slicing](#indexing)<br>\n",
    "3.2 [Constraints and Extraction](#constrain_extract)<br>\n",
    "3.3 [Merge](#merge)<br>\n",
    "3.4 [Concatenate](#concatenate)<br>\n",
    "3.5 [Summary of the Section](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0dev0\n",
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "print(iris.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 3.1 Indexing and Slicing<a id='indexing'></a>\n",
    "\n",
    "Cubes can be indexed in a familiar manner to that of NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "print(cube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air_potential_temperature / (K)     (time: 3; model_level_number: 4; grid_latitude: 20; grid_longitude: 10)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcube = cube[..., ::2, 15:35, :10]\n",
    "subcube.summary(shorten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the result of indexing a cube is *always* a copy and never a *view* on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration<a id='iteration'></a>\n",
    "\n",
    "We can loop through all desired subcubes in a larger cube using the cube methods ``slices`` and ``slices_over``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname,\n",
    "                      iris.Constraint('air_potential_temperature',\n",
    "                                      model_level_number=1))\n",
    "print(cube.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **``slices``** method returns all the slices of a cube on the dimensions specified by the coordinates passed to the slices method.\n",
    "\n",
    "So in this example, each `grid_latitude` / `grid_longitude` slice of the cube is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "for subcube in cube.slices(['grid_latitude', 'grid_longitude']):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **``slices_over``** to return one subcube for each coordinate value in a specified coordinate. This helps us when trying to retrieve all the slices along a given cube dimension.\n",
    "\n",
    "For example, let's consider retrieving all the slices over the time dimension (i.e. each time step in its own cube with a scalar time coordinate) using ``slices``. As per the above example, to achieve this using ``slices`` we would have to specify all the cube's dimensions _except_ the time dimension.\n",
    "\n",
    "Let's take a look at ``slices_over`` providing this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "for subcube in cube.slices_over('model_level_number'):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Indexing and slicing\n",
    "\n",
    "* What are the similarities between indexing and slicing?\n",
    "* What are the differences?\n",
    "* Which cube slicing method would be easiest to use to return all subcubes along the realization dimension?\n",
    "* Which cube slicing method would be easiest to use to return all horizontal 2D slices in a 4D cube?\n",
    "* In what situations would indexing be the best way to subset a cube? What about slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Constraints and Extraction<a id='constrain_extract'></a>\n",
    "\n",
    "We've already seen the basic ``load`` function, but we can also control which cubes are actually loaded with *constraints*. The simplest constraint is just a string, which filters cubes based on their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "print(iris.load(fname, 'air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris's constraints mechanism provides a powerful way to filter a subset of data from a larger collection. We've already seen that constraints can be used at load time to return data of interest from a file, but we can also apply constraints to a single cube, or a list of cubes, using their respective ``extract`` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "cubes = iris.load(fname)\n",
    "print(cubes.extract('air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest constraint, namely a string that matches a cube's name, is conveniently converted into an actual ``iris.Constraint`` instance wherever needed. However, we could construct this constraint manually and compare with the previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature')\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Constraint constructor also takes arbitrary keywords to constrain coordinate values. For example, to extract model level number 10 from the air potential temperature cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature',\n",
    "                                             model_level_number=10)\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of possible values, and even combine two constraints with ``&``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 2; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "print(cubes.extract('air_potential_temperature' & \n",
    "                    iris.Constraint(model_level_number=[4, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define arbitrary functions that operate on each cell of a coordinate. This is a common thing to do for floating point coordinates, where exact equality is non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "def less_than_10(cell):\n",
    "    \"\"\"Return True for values that are less than 10.\"\"\"\n",
    "    return cell < 10\n",
    "\n",
    "print(cubes.extract(iris.Constraint('air_potential_temperature',\n",
    "                                    model_level_number=less_than_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Constraints<a id='time_constraints'></a>\n",
    "\n",
    "It is common to want to build a constraint for time.  \n",
    "This can be achieved by comparing cells containing datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different approaches for producing time constraints in Iris. We will focus here on one approach for constraining on time in Iris. \n",
    "\n",
    "This approach allows us to access individual components of cell datetime objects and run comparisons on those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "time_constraint = iris.Constraint(time=lambda cell: cell.point.hour == 11)\n",
    "print(cube.extract(time_constraint).summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Cell methods are a part of cube metadata that record statistical operations that have been applied to a cube. For example, \"`mean: time (6hrs)`\" tells us that the cube has had a time mean over a 6hr interval applied.\n",
    "\n",
    "We can determine what, if any, cell methods a cube has with the attribute `cube.cell_methods`. The following function, then, tells us whether or not a cube has cell methods:\n",
    "\n",
    "```python\n",
    "def has_cell_methods(cube):\n",
    "    return len(cube.cell_methods) > 0\n",
    "```\n",
    "\n",
    "1\\. With the cubes loaded from ``[iris.sample_data_path('A1B_north_america.nc'), iris.sample_data_path('uk_hires.pp')]`` use the CubeList's **``extract``** method to filter only the cubes that have cell methods. (Hint: Look at the ``iris.Constraint`` documentation for the **cube_func** keyword). You should find that the 3 cubes are whittled down to just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the file found at ``iris.sample_data_path('A1B_north_america.nc')`` filter the cube, using constraints, such that only data between 1860 and 1980 remains (hint: This data has a 360-day calendar with yearly data from 1860 to 2100, so we will need to access the individual components of the cell point's datetime, to return a time dimension of length 120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Merge<a id='merge'></a>\n",
    "\n",
    "When Iris loads data it tries to reduce the number of cubes returned by collecting together multiple fields with\n",
    "shared metadata into a single multidimensional cube. In Iris, this is known as merging.\n",
    "\n",
    "In order to merge two cubes, they must be identical in everything but a scalar dimension, which goes on to become a new data dimension.\n",
    "\n",
    "The ``iris.load_raw`` function can be used as a diagnostic tool to identify the individual \"fields\" that Iris identifies in a given set of filenames before any merge takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_008.pp')\n",
    "raw_cubes = iris.load_raw(fname)\n",
    "\n",
    "print(len(raw_cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in detail at these cubes, we find that they are identical in every coordinate except for the scalar forecast_period and time coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_cubes[0])\n",
    "print('--' * 50)\n",
    "print(raw_cubes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any CubeList can be merged with the ``merge`` method, and the resulting CubeList from load_raw is no different.\n",
    "The ``merge`` method *always* returns another CubeList:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cube, = raw_cubes.merge()\n",
    "print(merged_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in more detail, we can see that the time coordinate has become a new dimension, as well as gaining another forecast_period auxiliary coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_cube.coord('time'))\n",
    "print(merged_cube.coord('forecast_period'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying merge problems\n",
    "\n",
    "In order to avoid the Iris merge functionality making often inappropriate assumptions about incoming data, merge is strict with regards to the uniformity of the incoming cubes.\n",
    "\n",
    "For example, if we load the fields from two ensemble members from the GloSea4 model sample data, we see we have 12 fields before any merge takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_00[34].pp')\n",
    "cubes = iris.load_raw(fname, 'surface_temperature')\n",
    "print(len(cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to merge these 12 cubes we get 2 cubes rather than one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_cubes = cubes.merge(unique=False)\n",
    "print(incomplete_cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in more detail at these two cubes, what is different between the two? (Hint: One value changes, another is completely missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incomplete_cubes[0])\n",
    "print('--' * 50)\n",
    "print(incomplete_cubes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the missing coordinate, we can trigger a merge of the 12 cubes into a single cube, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cube in cubes:\n",
    "    if not cube.coords('realization'):\n",
    "        cube.add_aux_coord(iris.coords.DimCoord(np.int32(3),\n",
    "                                                'realization'))\n",
    "\n",
    "merged_cubes = cubes.merge()\n",
    "print(merged_cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris includes functionality to simplify the identification process for causes of failed merges. The ``merge_cube`` method of a CubeList expects the list of cubes to contain only cubes that can be merged to produce a single cube. If they do not merge to a single cube, a descriptive exception will be raised. For instance:\n",
    "\n",
    "```\n",
    "   >>> cubes.merge_cube()\n",
    "   Traceback (most recent call last):\n",
    "     File \"<stdin>\", line 1, in <module>\n",
    "     ...\n",
    "   iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "     Coordinates in cube.aux_coords (scalar) differ: realization.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Concatenate<a id='concatenate'></a>\n",
    "\n",
    "We have seen that merge combines a list of cubes with a common scalar coordinate to produce a single cube with a new dimension created from these scalar values.\n",
    "\n",
    "But what happens if you try to combine cubes along a common dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('A1B_north_america.nc')\n",
    "cube = iris.load_cube(fname)\n",
    "\n",
    "cube_1 = cube[:10]\n",
    "cube_2 = cube[10:20]\n",
    "cubes = iris.cube.CubeList([cube_1, cube_2])\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cubes should be able to be merged; after all, they have both come from the same original cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cubes.merge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge cannot be used to combine common non-scalar coordinates. Instead we must use concatenate, which joins together (\"concatenates\") common non-scalar coordinates to produce a single cube with the common dimension extended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cubes.concatenate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with merge, Iris contains functionality to simplify the identification process for causes of failed concatenations. The ``concatenate_cube`` method of a CubeList expects the list of cubes to contain only cubes that can be concatenated to produce a single cube. If they do not concatenate to a single cube, a descriptive error will be raised. For instance:\n",
    "\n",
    "```\n",
    "    >>> print cubes.concatenate_cube()\n",
    "    Traceback (most recent call last):\n",
    "      ...\n",
    "    iris.exceptions.ConcatenateError: failed to concatenate into a single cube.\n",
    "      Scalar coordinates differ: forecast_reference_time, height != forecast_reference_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 : Solving Merge problems\n",
    "\n",
    "The following exercise is designed to give you experience of solving issues that prevent a merge from taking place.\n",
    "The output from ``merge_cube`` is included to help with identification, and once a fix has been identified, ``raw_cubes.merge()`` should result in a CubeList containing a single cube:\n",
    "\n",
    "The first exercise is completed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Identify and resolve the issue preventing the merge of ``air_potential_temperature`` cubes from ``resources/merge_exercise.1.*.nc``.\n",
    "\n",
    "    >>> raw_cubes = iris.load_raw('resources/merge_exercise.1.*.nc', 'air_potential_temperature')\n",
    "    >>> raw_cubes.merge_cube()\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "      cube.attributes keys differ: 'History'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cubes = iris.load_raw('resources/merge_exercise.1.*.nc', 'air_potential_temperature')\n",
    "\n",
    "# Print the attributes, clearly one is different.\n",
    "for cube in raw_cubes:\n",
    "    print(cube.attributes)\n",
    "\n",
    "# Remove the history attribute from the first cube.\n",
    "del raw_cubes[0].attributes['History']\n",
    "\n",
    "# Check that this has meant that a merge now results in a single cube.\n",
    "print('--' * 50)\n",
    "print(raw_cubes.merge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Identify and resolve the issue preventing the merge of ``air_potential_temperature`` cubes from ``resources/merge_exercise.5.*.nc`` (hint: can these cubes be merged?).\n",
    "\n",
    "    >>> raw_cubes = iris.load_raw('resources/merge_exercise.5.*.nc', 'air_potential_temperature')\n",
    "    >>> raw_cubes.merge_cube()\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "      Coordinates in cube.dim_coords differ: time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Section Summary : Cube Control<a id='summary'></a>\n",
    "\n",
    "In this section we learnt:\n",
    "* cubes can be indexed like arrays to produce sub-cubes\n",
    "* 'constraint' objects can be used to load only part of the data\n",
    "* particular methods are used to extract data by dates and times\n",
    "* Merging is used to join cubes into a larger combined dataset\n",
    "* Concatenation is a similar operation, used in slightly different circumstances to merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
